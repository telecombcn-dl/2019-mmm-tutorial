---
title: "home"
bg: white
color: black
style: center
---


{: .text-purple}
# Multimodal Deep Learning**
{: .text-purple}
## A tutorial of MMM 2019  
{: .text-purple}

### Thessaloniki, Greece (8th January 2019)

Deep neural networks have boosted the convergence of multimedia data analytics in a unified framework shared by practitioners in natural language, vision and speech. Image captioning, lip reading or video sonorization are some of the first applications of a new and exciting field of research exploiting the generalization properties of deep neural representation. This tutorial will firstly review the basic neural architectures to encode and decode vision, text and audio, to later review the those models that have successfully translated information across modalities.
